# 逻辑回归原理

$P_{label=1} = \frac{1}{1+e^{-(\beta_0+\beta_1x)}}$ , 可见逻辑回归本质还是线性方程，它额输出值永远除以(0,1)区间，吻合了概率的特性，并且当线性方程的输出变大，函数就越接近1.

## 损失函数

$L = \sum^N_{i=1}[y_ilog(p_i)+(1-y_i)log(1-p_i)] , p_i 是模型预测为1的概率$ ， 目标是最大化这个值。

所以逻辑回归的损失函数和线性回归截然不同，它的参数也不是一次就得来的，需要迭代

常用于逻辑回归的优化方法有：牛顿法

## 牛顿法原理详解

$\beta^(t+1)=\beta^t-H^{-1}\nabla f(\beta^t)$

$\beta^t 是当前参数值，\nabla f(\beta^t)是目标函数的一阶导数，H：目标函数f(\beta)的海森(Hessian Matrix)矩阵（二阶导数矩阵，描述函数的曲率）$

为了更好的理解这个公式，现在降维到1维

泰勒series,函数在a点时的值可以近似为: $f(x) \approx f(a)+f'(a)(x-a)+\frac{1}{2}f''(a)(x-a)^2$

对这个系列，对X求导，得到 $f'(a)+f''(a)(x-a)=0$

求解得到 $x = a-\frac{f'(a)}{f''(a)}, 这也就是牛顿优化的更新公式x_{k+1} = x_k-\frac{f'(x_k)}{f''(x_k)} $

这个公式会自动朝着极值点移动，因为根据上面证明，它本身就是求导后设右边为0，也就是极值点。

同样的道理，$\beta^(t+1)=\beta^t-H^{-1}\nabla f(\beta^t)$ 这个公式其实和一维公式是一样的，只是把二阶导数在 $x_k$点的值替换成了Hessian矩阵，也就是多个变量的二阶导的矩阵，把一阶导数的值换成了向量。

